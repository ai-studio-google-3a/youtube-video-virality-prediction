{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HRkiNnE2Rb92"
      },
      "source": [
        "# **Google Team 3A Google Colab Notebook**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52zG-7piJ0wp"
      },
      "source": [
        "Import Datatset and Tools"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rhKKQOPTRfJP",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        },
        "outputId": "d5613842-3a5f-4d26-bd8f-c162e12a52e5"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'US_youtube_trending_data.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-deabeb9a436f>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# # Load Dataset Here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"US_youtube_trending_data.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon_bad_lines\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'skip'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'python'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;31m#The on_bad_lines argument will skip any malformed lines and the engine argument set to python will use the python parsing engine, which is more flexible\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'US_youtube_trending_data.csv'"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from google.colab import files\n",
        "from datetime import datetime\n",
        "\n",
        "# # Load Dataset Here\n",
        "df = pd.read_csv(\"US_youtube_trending_data.csv\", on_bad_lines='skip', engine='python')\n",
        "#The on_bad_lines argument will skip any malformed lines and the engine argument set to python will use the python parsing engine, which is more flexible\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pjZIJwhoJ6ku"
      },
      "source": [
        "# Exploratory Data Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qOetIBy9RiMq"
      },
      "outputs": [],
      "source": [
        "print(df.shape)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WD6KtqxuKCDZ"
      },
      "outputs": [],
      "source": [
        "# Delete Features that are not needed\n",
        "\n",
        "df = df.drop(columns=['tags'])\n",
        "df = df.drop(columns=['video_id'])\n",
        "df = df.drop(columns=['channelId'])\n",
        "df = df.drop(columns=['thumbnail_link'])\n",
        "\n",
        "# Cleaning (removing videos with comments and ratings disabled)\n",
        "\n",
        "df = df.drop(df[df['view_count'] == 0].index)\n",
        "df = df[df['comments_disabled'] == False]\n",
        "df = df[df['ratings_disabled'] == False]\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Txc_9QQPKUet"
      },
      "outputs": [],
      "source": [
        "def calculate_time_to_trend(row):\n",
        "    trending_date = datetime.strptime(row['trending_date'], '%Y-%m-%dT%H:%M:%SZ')\n",
        "    published_at = datetime.strptime(row['publishedAt'], '%Y-%m-%dT%H:%M:%SZ')\n",
        "    time_difference = (trending_date - published_at).total_seconds() / 60  # Convert to minutes\n",
        "    return time_difference\n",
        "\n",
        "df['time_to_trend_minutes'] = df.apply(calculate_time_to_trend, axis=1)\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uYJ4AHkNoUyR"
      },
      "outputs": [],
      "source": [
        "# categoryId vs view_count\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "df.plot(kind='scatter', x='categoryId', y='view_count', s=32, alpha=.8)\n",
        "plt.gca().spines[['top', 'right',]].set_visible(False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FdJjFt5qmk5q"
      },
      "outputs": [],
      "source": [
        "# Comment vs View Count\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "# Create the scatter plot\n",
        "df.plot(kind='scatter', x='comment_count', y='view_count', s=32, alpha=.8)\n",
        "\n",
        "# Add labels to the points\n",
        "for i, row in df.iterrows():  # Iterate over each row of the DataFrame\n",
        "    plt.annotate(row['title'], (row['comment_count'], row['view_count']))\n",
        "\n",
        "# Customize the plot appearance\n",
        "plt.gca().spines[['top', 'right',]].set_visible(False)\n",
        "plt.xlabel(\"Comment Count\")\n",
        "plt.ylabel(\"View Count\")\n",
        "plt.title(\"Comment Count vs. View Count\")\n",
        "\n",
        "# Show the plot\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U8WTwMEz6C6X"
      },
      "outputs": [],
      "source": [
        "# Time of Day\n",
        "\n",
        "# 0 = morning, 1 = afternoon, 2 = night\n",
        "def calculate_time_of_day(row):\n",
        "    published_at = datetime.strptime(row['publishedAt'], '%Y-%m-%dT%H:%M:%SZ')\n",
        "    time_difference = (published_at).hour\n",
        "    if (time_difference < 10):\n",
        "        return 0\n",
        "    elif (time_difference < 17):\n",
        "        return 1\n",
        "    return 2;\n",
        "\n",
        "df['time_posted'] = df.apply(calculate_time_of_day, axis=1)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Calculate Engagement Features"
      ],
      "metadata": {
        "id": "tka1CD4qIjJB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JvIemaA1BfXF"
      },
      "outputs": [],
      "source": [
        "# Calculate Engagement Rate (Likes + Dislikes + No. of Comments / View Count)\n",
        "\n",
        "df['engagement_rate'] = round((df['likes'] + df['comment_count']) / df['view_count'])\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Like - Dislike Ratio\n",
        "# df['like_dislike_ratio'] = df['likes'] / (df['dislikes'])\n",
        "\n",
        "# Comment - View Ratio\n",
        "df['comment_view_ratio'] = round(df['comment_count'] / df['view_count'])\n",
        "\n",
        "df.head()"
      ],
      "metadata": {
        "id": "Gp2txBvWFRBn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define Viral vs. Trending Thresholds"
      ],
      "metadata": {
        "id": "KZOzgtkcItRF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define thresholds for trending & viral videos\n",
        "# viral = 1,000,000 views in 24 hours\n",
        "viral_threshold = 1000000\n",
        "\n",
        "# Create trending/viral labels (1 = is viral/trending, 0 = isn't viral/trending)\n",
        "df['is_viral'] = (df['view_count'] >= viral_threshold).astype(int)\n",
        "\n",
        "df.head()"
      ],
      "metadata": {
        "id": "ki-VQn5tGvGm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#*---- Next steps: split data into training/testing sets ?*\n"
      ],
      "metadata": {
        "id": "Mf2M-Nl8I9j1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Split Data (Training vs. Testing Sets)"
      ],
      "metadata": {
        "id": "n1YKMOusIz-y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "5HkL6etzv0gP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Next steps - split data into training/testing sets ?\n",
        "missing_values = df.isnull().sum()\n",
        "print(\"Missing values in each column:\\n\", missing_values)\n",
        "df = df.dropna()\n",
        "\n",
        "\n",
        "# Select features to use in the model (try to use clustering?)\n",
        "X = df[['time_to_trend_minutes', 'time_posted', 'likes', 'comment_view_ratio']]\n",
        "y = df['view_count']\n",
        "\n",
        "# # Split the data (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)"
      ],
      "metadata": {
        "id": "C7afiBerH291"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Linear Regression\n",
        "model = LinearRegression()\n",
        "model.fit(X_train, y_train)\n",
        "prediction = model.predict(X_test)"
      ],
      "metadata": {
        "id": "M9cUlpZTusRv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Metrics\n",
        "\n",
        "# Print mean squared error\n",
        "print('\\nModel Performance\\n\\nRMSE =   %.2f'\n",
        "      % np.sqrt(mean_squared_error(y_test, prediction)))\n",
        "# The coefficient of determination: 1 is perfect prediction\n",
        "print(' R^2 =   %.2f'\n",
        "      % r2_score(y_test, prediction))"
      ],
      "metadata": {
        "id": "jMCrRfjjwo1X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualization\n",
        "plt.plot(X_test, prediction, color='blue', linewidth=3);\n",
        "\n",
        "plt.xlabel('real view_count');\n",
        "plt.ylabel('view_count prediction');"
      ],
      "metadata": {
        "id": "4ma6UGclvnq4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "missing_values = df.isnull().sum()\n",
        "print(\"Missing values in each column:\\n\", missing_values)\n",
        "X.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "df = df.dropna()\n",
        "\n",
        "\n",
        "# # Next steps - split data into training/testing sets ?\n",
        "\n",
        "# Select features to use in the model\n",
        "X = df[[ 'engagement_rate', 'comment_view_ratio']]\n",
        "print(X)\n",
        "\n",
        "y = df['is_viral']\n",
        "\n",
        "# # Split the data (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)"
      ],
      "metadata": {
        "id": "V2v2UfaWxZ1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Logistic Regression\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "\n",
        "# Initialize the logistic regression model\n",
        "log_reg = LogisticRegression()\n",
        "\n",
        "# Fit the model on the training data\n",
        "log_reg.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test data\n",
        "y_pred = log_reg.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "# Print classification report and confusion matrix\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
        "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred))"
      ],
      "metadata": {
        "id": "kbcGNZ34uuRn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Decision Tree\n",
        "from sklearn.tree import DecisionTreeClassifier"
      ],
      "metadata": {
        "id": "e240ZgyRD5Q1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Create the  DecisionTreeClassifier model object below and assign to variable 'model'\n",
        "model3 = DecisionTreeClassifier(max_depth=16, min_samples_leaf=1)\n",
        "\n",
        "# 2. Fit the model to the training data below\n",
        "model3.fit(X_train, y_train)\n",
        "\n",
        "# 3. Make predictions on the test data below and assign the result to the variable 'class_label_predictions'\n",
        "class_label_predictions = model3.predict(X_test)\n",
        "\n",
        "# 4. Compute the accuracy here and save the result to the variable 'acc_score'\n",
        "acc_score = accuracy_score(y_test, class_label_predictions)\n",
        "print(acc_score)"
      ],
      "metadata": {
        "id": "aDAFr30qEhDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Maybe Unsuperised Learning to find features? KMeans Clustering\n",
        "\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "\n"
      ],
      "metadata": {
        "id": "BsGugbWjGUv-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Which features to use for clustering\n",
        "cluster_features = df[['time_to_trend_minutes', 'engagement_rate', 'comment_view_ratio', 'likes']]\n",
        "\n",
        "# standardize features\n",
        "scaler = StandardScaler()\n",
        "cluster_features_scaled = scaler.fit_transform(cluster_features)\n",
        "\n",
        "# define the number of clusters\n",
        "n_clusters = 3\n",
        "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
        "\n",
        "\n",
        "kmeans.fit(cluster_features_scaled)\n",
        "\n",
        "# assign cluster labels to each data point\n",
        "df['cluster'] = kmeans.labels_\n",
        "\n",
        "# plot\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.scatter(df['time_to_trend_minutes'], df['engagement_rate'], c=df['cluster'], cmap='viridis', s=50)\n",
        "plt.xlabel('Time to Trend (minutes)')\n",
        "plt.ylabel('Engagement Rate')\n",
        "plt.title(f'KMeans Clustering with {n_clusters} Clusters')\n",
        "plt.colorbar(label='Cluster')\n",
        "plt.gca().spines[['top', 'right']].set_visible(False)\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "P1RCvQbHl1x6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# NLP - Title\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9LRzFeeGabTm"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}